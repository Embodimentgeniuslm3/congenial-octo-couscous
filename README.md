# fantastic-tribble
&lt;?xml version="1.0" encoding="utf-8"?> &lt;rss xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:status="https://status.dev.azure.com/" version="2.0">   &lt;channel>     &lt;title>Azure DevOps - Recent Events&lt;/title>     &lt;link>https://status.dev.azure.com/&lt;/link>     &lt;atom:link rel="self" type="application/rss+xml" href="https://status.dev.azure.com/_rss" />     &lt;atom:link rel="alternate" type="text/html" href="https://status.dev.azure.com/" />     &lt;description>Recent events affecting availability of Azure DevOps Services.&lt;/description>     &lt;language>en-US&lt;/language>     &lt;pubDate>Wed, 02 Jun 2021 06:53:12 GMT&lt;/pubDate>     &lt;lastBuildDate>Wed, 02 Jun 2021 06:53:12 GMT&lt;/lastBuildDate>     &lt;sy:updatePeriod>hourly&lt;/sy:updatePeriod>     &lt;sy:updateFrequency>1&lt;/sy:updateFrequency>     &lt;item>       &lt;title>Errors or slow responses for some customers&lt;/title>       &lt;link>https://status.dev.azure.com/_event/243715681&lt;/link>       &lt;pubDate>Wed, 02 Jun 2021 06:53:12 GMT&lt;/pubDate>       &lt;atom:updated>2021-06-02T06:53:12.470Z&lt;/atom:updated>       &lt;dc:creator>Richa Kumar (Azure DevOps)&lt;/dc:creator>       &lt;status:geography>Europe&lt;/status:geography>       &lt;status:state>Resolved&lt;/status:state>       &lt;guid isPermaLink="true">https://status.dev.azure.com/_event/243715681&lt;/guid>       &lt;description>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Richa Kumar, 6/2/2021 6:53:12 AM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Some customers may have experienced errors or slow responses. The issue has been mitigated for now. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.&amp;lt;/p&amp;gt;  &lt;/description>       &lt;content:encoded>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Richa Kumar, 6/2/2021 6:53:12 AM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Some customers may have experienced errors or slow responses. The issue has been mitigated for now. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.&amp;lt;/p&amp;gt;  &amp;lt;hr&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Initial communication&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;mauro ottaviani, 6/2/2021 5:40:06 AM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.&amp;lt;/p&amp;gt;  &lt;/content:encoded>     &lt;/item>     &lt;item>       &lt;title>Artifacts Availability Degradation in Central US&lt;/title>       &lt;link>https://status.dev.azure.com/_event/242638829&lt;/link>       &lt;pubDate>Tue, 25 May 2021 17:23:04 GMT&lt;/pubDate>       &lt;atom:updated>2021-05-25T17:23:04.100Z&lt;/atom:updated>       &lt;dc:creator>Adam Barr (VSNC) (Azure DevOps)&lt;/dc:creator>       &lt;status:geography>United States&lt;/status:geography>       &lt;status:state>Resolved&lt;/status:state>       &lt;guid isPermaLink="true">https://status.dev.azure.com/_event/242638829&lt;/guid>       &lt;description>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Adam Barr (VSNC), 5/25/2021 5:23:04 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Some requests to a single instance of Azure Artifacts in Central US will time out or experience performance issues caused by capacity degradation.  We've mitigated this and confirmed performance and availability are healthy again.  As a follow-up, we're evaluating the cause of the compute capacity degradation and the speed of our automated scale-up.&amp;lt;/p&amp;gt;  &lt;/description>       &lt;content:encoded>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Adam Barr (VSNC), 5/25/2021 5:23:04 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Some requests to a single instance of Azure Artifacts in Central US will time out or experience performance issues caused by capacity degradation.  We've mitigated this and confirmed performance and availability are healthy again.  As a follow-up, we're evaluating the cause of the compute capacity degradation and the speed of our automated scale-up.&amp;lt;/p&amp;gt;  &amp;lt;hr&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Initial communication&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;mauro ottaviani, 5/25/2021 4:43:04 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.&amp;lt;/p&amp;gt;  &lt;/content:encoded>     &lt;/item>     &lt;item>       &lt;title>Web interface load-time degradation &lt;/title>       &lt;link>https://status.dev.azure.com/_event/243607448&lt;/link>       &lt;pubDate>Tue, 01 Jun 2021 14:44:04 GMT&lt;/pubDate>       &lt;atom:updated>2021-06-01T14:44:04.060Z&lt;/atom:updated>       &lt;dc:creator>Zachariah Cox (Azure DevOps)&lt;/dc:creator>       &lt;status:geography>United States&lt;/status:geography>       &lt;status:state>Resolved&lt;/status:state>       &lt;guid isPermaLink="true">https://status.dev.azure.com/_event/243607448&lt;/guid>       &lt;description>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Zachariah Cox, 6/1/2021 2:39:31 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;We had a short period of time during our virtual ip swaps where the data providers for our web interfaces were slow. This availability degradation repaired recovered shortly after the swap completed. Our engineers are investigating the telemetry to avoid such temporary outages in the future.&amp;lt;/p&amp;gt;  &lt;/description>       &lt;content:encoded>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Zachariah Cox, 6/1/2021 2:39:31 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;We had a short period of time during our virtual ip swaps where the data providers for our web interfaces were slow. This availability degradation repaired recovered shortly after the swap completed. Our engineers are investigating the telemetry to avoid such temporary outages in the future.&amp;lt;/p&amp;gt;  &amp;lt;hr&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Initial communication&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;mauro ottaviani, 6/1/2021 1:58:05 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.&amp;lt;/p&amp;gt;  &lt;/content:encoded>     &lt;/item>     &lt;item>       &lt;title>Service Hooks failing to deliver&lt;/title>       &lt;link>https://status.dev.azure.com/_event/243971118&lt;/link>       &lt;pubDate>Thu, 03 Jun 2021 22:26:23 GMT&lt;/pubDate>       &lt;atom:updated>2021-06-03T22:26:23.050Z&lt;/atom:updated>       &lt;dc:creator>Zachariah Cox (Azure DevOps)&lt;/dc:creator>       &lt;status:geography>United States&lt;/status:geography>       &lt;status:state>Resolved&lt;/status:state>       &lt;guid isPermaLink="true">https://status.dev.azure.com/_event/243971118&lt;/guid>       &lt;description>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Zachariah Cox, 6/3/2021 10:26:23 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;The issue is now fully mitigated. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.&amp;lt;/p&amp;gt;  &lt;/description>       &lt;content:encoded>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Zachariah Cox, 6/3/2021 10:26:23 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;The issue is now fully mitigated. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.&amp;lt;/p&amp;gt;  &amp;lt;hr&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Initial communication&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Zachariah Cox, 6/3/2021 9:57:11 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Our engineers are currently investigating an event impacting Azure DevOps.&amp;lt;br /&amp;gt; A small percentage of all outgoing service hooks notifications are failing to deliver in central us region.  The event is being triaged and we will post an update as soon as we know more.&amp;lt;/p&amp;gt;  &lt;/content:encoded>     &lt;/item>     &lt;item>       &lt;title>Minor performance degradation &lt;/title>       &lt;link>https://status.dev.azure.com/_event/243940440&lt;/link>       &lt;pubDate>Thu, 03 Jun 2021 17:32:08 GMT&lt;/pubDate>       &lt;atom:updated>2021-06-03T17:32:08.173Z&lt;/atom:updated>       &lt;dc:creator>Zachariah Cox (Azure DevOps)&lt;/dc:creator>       &lt;status:geography>United States&lt;/status:geography>       &lt;status:state>Resolved&lt;/status:state>       &lt;guid isPermaLink="true">https://status.dev.azure.com/_event/243940440&lt;/guid>       &lt;description>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Zachariah Cox, 6/3/2021 5:32:08 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;The issue is now fully mitigated. This issue had a minor impact fewer than .5% of users. We are working to tune this automated alert to be less sensitive.&amp;lt;/p&amp;gt;  &lt;/description>       &lt;content:encoded>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Zachariah Cox, 6/3/2021 5:32:08 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;The issue is now fully mitigated. This issue had a minor impact fewer than .5% of users. We are working to tune this automated alert to be less sensitive.&amp;lt;/p&amp;gt;  &amp;lt;hr&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Initial communication&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;mauro ottaviani, 6/3/2021 4:50:09 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.&amp;lt;/p&amp;gt;  &lt;/content:encoded>     &lt;/item>     &lt;item>       &lt;title>Minor performance degradation on some rest API calls. &lt;/title>       &lt;link>https://status.dev.azure.com/_event/243921862&lt;/link>       &lt;pubDate>Thu, 03 Jun 2021 15:14:05 GMT&lt;/pubDate>       &lt;atom:updated>2021-06-03T15:14:05.477Z&lt;/atom:updated>       &lt;dc:creator>Zachariah Cox (Azure DevOps)&lt;/dc:creator>       &lt;status:geography>United States&lt;/status:geography>       &lt;status:state>Resolved&lt;/status:state>       &lt;guid isPermaLink="true">https://status.dev.azure.com/_event/243921862&lt;/guid>       &lt;description>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Zachariah Cox, 6/3/2021 3:14:05 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;The issue is now fully mitigated. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.&amp;lt;/p&amp;gt;  &lt;/description>       &lt;content:encoded>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Zachariah Cox, 6/3/2021 3:14:05 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;The issue is now fully mitigated. Our engineers will be investigating this further to learn from and reduce the risk of potential recurrences. We apologize for the impact this had on our customers.&amp;lt;/p&amp;gt;  &amp;lt;hr&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Initial communication&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;mauro ottaviani, 6/3/2021 1:59:01 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.&amp;lt;/p&amp;gt;  &lt;/content:encoded>     &lt;/item>     &lt;item>       &lt;title>Test Alert&lt;/title>       &lt;link>https://status.dev.azure.com/_event/243286815&lt;/link>       &lt;pubDate>Sat, 29 May 2021 21:23:31 GMT&lt;/pubDate>       &lt;atom:updated>2021-05-29T21:23:31.370Z&lt;/atom:updated>       &lt;dc:creator>Adam Barr (VSNC) (Azure DevOps)&lt;/dc:creator>       &lt;status:geography>United States&lt;/status:geography>       &lt;status:state>Resolved&lt;/status:state>       &lt;guid isPermaLink="true">https://status.dev.azure.com/_event/243286815&lt;/guid>       &lt;description>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Adam Barr (VSNC), 5/29/2021 9:23:31 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;This was another recurrence of a non-production monitor that fired before our changes to suppress them.  We apologize for the noise.&amp;lt;/p&amp;gt;  &lt;/description>       &lt;content:encoded>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Adam Barr (VSNC), 5/29/2021 9:23:31 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;This was another recurrence of a non-production monitor that fired before our changes to suppress them.  We apologize for the noise.&amp;lt;/p&amp;gt;  &amp;lt;hr&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Initial communication&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;mauro ottaviani, 5/29/2021 9:12:57 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.&amp;lt;/p&amp;gt;  &lt;/content:encoded>     &lt;/item>     &lt;item>       &lt;title>Test Alert&lt;/title>       &lt;link>https://status.dev.azure.com/_event/243283973&lt;/link>       &lt;pubDate>Sat, 29 May 2021 21:03:54 GMT&lt;/pubDate>       &lt;atom:updated>2021-05-29T21:03:54.100Z&lt;/atom:updated>       &lt;dc:creator>Adam Barr (VSNC) (Azure DevOps)&lt;/dc:creator>       &lt;status:geography>United States&lt;/status:geography>       &lt;status:state>Resolved&lt;/status:state>       &lt;guid isPermaLink="true">https://status.dev.azure.com/_event/243283973&lt;/guid>       &lt;description>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Adam Barr (VSNC), 5/29/2021 9:03:54 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;A monitor in a non-production instance fired and triggered this status update.  There's no customer impact and we've updated the monitor to not trigger status updates going forward.&amp;lt;/p&amp;gt;  &lt;/description>       &lt;content:encoded>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;Adam Barr (VSNC), 5/29/2021 9:03:54 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;A monitor in a non-production instance fired and triggered this status update.  There's no customer impact and we've updated the monitor to not trigger status updates going forward.&amp;lt;/p&amp;gt;  &amp;lt;hr&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Initial communication&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;mauro ottaviani, 5/29/2021 8:32:38 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.&amp;lt;/p&amp;gt;  &lt;/content:encoded>     &lt;/item>     &lt;item>       &lt;title>Test Alert&lt;/title>       &lt;link>https://status.dev.azure.com/_event/243157146&lt;/link>       &lt;pubDate>Fri, 28 May 2021 22:53:20 GMT&lt;/pubDate>       &lt;atom:updated>2021-05-28T22:53:20.203Z&lt;/atom:updated>       &lt;dc:creator>mauro ottaviani (Azure DevOps)&lt;/dc:creator>       &lt;status:geography>United States&lt;/status:geography>       &lt;status:state>Resolved&lt;/status:state>       &lt;guid isPermaLink="true">https://status.dev.azure.com/_event/243157146&lt;/guid>       &lt;description>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;mauro ottaviani, 5/28/2021 10:43:45 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;This event was created in error by a faulty monitor. There has actually been no customer impact from this event and we're updating the monitor to correct it to avoid noise in the future.&amp;lt;/p&amp;gt;  &lt;/description>       &lt;content:encoded>&amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Final update&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;mauro ottaviani, 5/28/2021 10:43:45 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;This event was created in error by a faulty monitor. There has actually been no customer impact from this event and we're updating the monitor to correct it to avoid noise in the future.&amp;lt;/p&amp;gt;  &amp;lt;hr&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Initial communication&amp;lt;/strong&amp;gt;&amp;lt;br&amp;gt;mauro ottaviani, 5/28/2021 8:17:40 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Our engineers are currently investigating an event impacting Azure DevOps. The event is being triaged and we will post an update as soon as we know more.&amp;lt;/p&amp;gt;  &lt;/content:encoded>     &lt;/item>     &lt;item>       &lt;title>Hosted Agents not being assigned&lt;/title>       &lt;link>https://status.dev.azure.com/_event/243119630/post-mortem&lt;/link>       &lt;pubDate>Fri, 04 Jun 2021 16:24:42 GMT&lt;/pubDate>       &lt;atom:updated>2021-06-04T16:24:42.520Z&lt;/atom:updated>       &lt;dc:creator>Vladimir Sebesta (Azure DevOps)&lt;/dc:creator>       &lt;status:geography>Asia Pacific&lt;/status:geography>       &lt;status:geography>Australia&lt;/status:geography>       &lt;status:geography>Brazil&lt;/status:geography>       &lt;status:geography>Canada&lt;/status:geography>       &lt;status:geography>Europe&lt;/status:geography>       &lt;status:geography>India&lt;/status:geography>       &lt;status:geography>United Kingdom&lt;/status:geography>       &lt;status:geography>United States&lt;/status:geography>       &lt;status:state>Resolved&lt;/status:state>       &lt;guid isPermaLink="true">https://status.dev.azure.com/_event/243119630/post-mortem&lt;/guid>       &lt;description>&amp;lt;p&amp;gt;Vladimir Sebesta, 6/4/2021 4:21:21 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;On May 28, 2021, Azure DevOps customers experienced a major global outage of hosted agents, with customers not able to run jobs on hosted agents between 15:17 UTC and 19:20 UTC. An availability issue in one scale unit of the service that manages hosted agents caused a global availability problem for agent assignments.  This was caused by a scale unit which became unhealthy. This resulted in a circuit breaker opening, which wasn’t scoped correctly causing broad impact. When we tried to mitigate by removing the faulty scale unit out of rotation, we still had cached values which required manual intervention to restore agent pool availability.&amp;lt;/p&amp;gt; &amp;lt;h2&amp;gt;Customer Impact&amp;lt;/h2&amp;gt; &amp;lt;p&amp;gt;Between 15:17 UTC and 19:20 UTC we saw 12,859 organizations that had abandoned requests for hosted agents or long hosted agent queue times.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Number of distinct orgs per minute impacted by &amp;amp;gt;5s queue time:&amp;lt;/strong&amp;gt;&amp;lt;br /&amp;gt; &amp;lt;img src="https://status.dev.azure.com/_apis/status/livesiteevents/243119630/attachments/6390a369-8795-4f9a-bb73-e2384112a47b.png" alt="Chart depicting data described in this report." /&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Accounts with abandoned jobs per minute&amp;lt;/strong&amp;gt; -- there’s a 45 minute initial delay before jobs are abandoned, which corresponds to the outage start time of 15:17 UTC:&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;img src="https://status.dev.azure.com/_apis/status/livesiteevents/243119630/attachments/d678cb69-3082-4d34-88a6-a0b5c2a2ddcd.png" alt="Chart depicting data described in this report." /&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;h2&amp;gt;What Happened&amp;lt;/h2&amp;gt; &amp;lt;p&amp;gt;When Azure Pipelines gets a request to run a job, we query all our machine management service (MMS) scale units in the region to determine which scale units have available capacity to run the job. On May 28th 15:17 UTC, we had an outage of one of our canary scale units related to a critical dependency.  Canary is an environment that is expected to be less stable, but does not serve any production traffic.  While the team was investigating and escalating the outage in canary, we received automated alerts on hosted agent availability in other scale units and regions.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Typically, that incident would have zero impact on customers. Unfortunately, there were a few factors that led to the broad impact and the slow mitigation. A service architecture change was rolled out recently that introduced an incorrectly scoped circuit breaker. When calls to the canary failed and the breaker opened, calls to all MMS scale units were short-circuited instead of just the canary. At 16:25 UTC we force closed the circuit breaker so that we could continue sending requests to other scale units. Once this was rolled out we still saw issues because now calls were going through to the unhealthy canary scale, and these calls were wrapped with a retry handler with exponential backoff. That caused delays in allowing us to analyze available capacity across the region and in turn delayed assigning jobs to hosted agents.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;It became clear at this point that we needed to stop calling the canary scale unit. At 17:30 UTC, we removed the canary from our list of scale units. This information is cached in a number of places and the cache was not flushed, so at 18:15 UTC we also started a manual recycle of service VMs to reset the cache. As the recycles completed, we started seeing recovery at 18:20 UTC, with the incident fully mitigated at 19:20 UTC.&amp;lt;/p&amp;gt; &amp;lt;h2&amp;gt;Next Steps&amp;lt;/h2&amp;gt; &amp;lt;p&amp;gt;One scale unit having issues should never impact other scale units. That’s one of the key reasons to have scale units, after all. We have a number of fixes we are working on the backend to fix this, some of them being more immediate like enforcing a stricter timeout while reaching out to our Scale Units, fixing all the circuit breakers so they do not have global impact. That work is already in progress. We also plan to do game days to simulate scenarios of SU failures to ensure we can handle that scenario gracefully in the future.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;We also plan to take a number of action items to improve our time to recovery. During the incident the fixes were rolled out in a phased manner with baked in delays. We want to add the ability to roll out mitigations more safely and broadly.&amp;lt;br /&amp;gt; We recognize that hosted agent availability and job success is critical for Azure Pipelines customers and sincerely apologize for the disruption this outage caused.  We learned a lot from this and are making improvements to prevent a recurrence.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Sincerely,&amp;lt;br /&amp;gt; The Azure DevOps Team&amp;lt;/p&amp;gt;  &lt;/description>       &lt;content:encoded>&amp;lt;p&amp;gt;Vladimir Sebesta, 6/4/2021 4:21:21 PM&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;On May 28, 2021, Azure DevOps customers experienced a major global outage of hosted agents, with customers not able to run jobs on hosted agents between 15:17 UTC and 19:20 UTC. An availability issue in one scale unit of the service that manages hosted agents caused a global availability problem for agent assignments.  This was caused by a scale unit which became unhealthy. This resulted in a circuit breaker opening, which wasn’t scoped correctly causing broad impact. When we tried to mitigate by removing the faulty scale unit out of rotation, we still had cached values which required manual intervention to restore agent pool availability.&amp;lt;/p&amp;gt; &amp;lt;h2&amp;gt;Customer Impact&amp;lt;/h2&amp;gt; &amp;lt;p&amp;gt;Between 15:17 UTC and 19:20 UTC we saw 12,859 organizations that had abandoned requests for hosted agents or long hosted agent queue times.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Number of distinct orgs per minute impacted by &amp;amp;gt;5s queue time:&amp;lt;/strong&amp;gt;&amp;lt;br /&amp;gt; &amp;lt;img src="https://status.dev.azure.com/_apis/status/livesiteevents/243119630/attachments/6390a369-8795-4f9a-bb73-e2384112a47b.png" alt="Chart depicting data described in this report." /&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;strong&amp;gt;Accounts with abandoned jobs per minute&amp;lt;/strong&amp;gt; -- there’s a 45 minute initial delay before jobs are abandoned, which corresponds to the outage start time of 15:17 UTC:&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;img src="https://status.dev.azure.com/_apis/status/livesiteevents/243119630/attachments/d678cb69-3082-4d34-88a6-a0b5c2a2ddcd.png" alt="Chart depicting data described in this report." /&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;h2&amp;gt;What Happened&amp;lt;/h2&amp;gt; &amp;lt;p&amp;gt;When Azure Pipelines gets a request to run a job, we query all our machine management service (MMS) scale units in the region to determine which scale units have available capacity to run the job. On May 28th 15:17 UTC, we had an outage of one of our canary scale units related to a critical dependency.  Canary is an environment that is expected to be less stable, but does not serve any production traffic.  While the team was investigating and escalating the outage in canary, we received automated alerts on hosted agent availability in other scale units and regions.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Typically, that incident would have zero impact on customers. Unfortunately, there were a few factors that led to the broad impact and the slow mitigation. A service architecture change was rolled out recently that introduced an incorrectly scoped circuit breaker. When calls to the canary failed and the breaker opened, calls to all MMS scale units were short-circuited instead of just the canary. At 16:25 UTC we force closed the circuit breaker so that we could continue sending requests to other scale units. Once this was rolled out we still saw issues because now calls were going through to the unhealthy canary scale, and these calls were wrapped with a retry handler with exponential backoff. That caused delays in allowing us to analyze available capacity across the region and in turn delayed assigning jobs to hosted agents.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;It became clear at this point that we needed to stop calling the canary scale unit. At 17:30 UTC, we removed the canary from our list of scale units. This information is cached in a number of places and the cache was not flushed, so at 18:15 UTC we also started a manual recycle of service VMs to reset the cache. As the recycles completed, we started seeing recovery at 18:20 UTC, with the incident fully mitigated at 19:20 UTC.&amp;lt;/p&amp;gt; &amp;lt;h2&amp;gt;Next Steps&amp;lt;/h2&amp;gt; &amp;lt;p&amp;gt;One scale unit having issues should never impact other scale units. That’s one of the key reasons to have scale units, after all. We have a number of fixes we are working on the backend to fix this, some of them being more immediate like enforcing a stricter timeout while reaching out to our Scale Units, fixing all the circuit breakers so they do not have global impact. That work is already in progress. We also plan to do game days to simulate scenarios of SU failures to ensure we can handle that scenario gracefully in the future.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;We also plan to take a number of action items to improve our time to recovery. During the incident the fixes were rolled out in a phased manner with baked in delays. We want to add the ability to roll out mitigations more safely and broadly.&amp;lt;br /&amp;gt; We recognize that hosted agent availability and job success is critical for Azure Pipelines customers and sincerely apologize for the disruption this outage caused.  We learned a lot from this and are making improvements to prevent a recurrence.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;Sincerely,&amp;lt;br /&amp;gt; The Azure DevOps Team&amp;lt;/p&amp;gt;  &lt;/content:encoded>     &lt;/item>   &lt;/channel> &lt;/rss>
